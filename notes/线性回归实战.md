## 线性回归实战

#### tensor和numpy

```
tensor数据结构更复杂，需要的空间更大，tensor数据支持自动微分

numpy一般是数组对象

tensor支持GPU计算，numpy不支持GPU计算
```



#### 计算图

```
计算图分为静态图和动态图。静态图，顾名思义就是图是确定的，即整个运算过程预先定义好了，然后再次运行的过程中只运算而不再搭建计算图，看起来就是数据在规定的图中流动。动态图，就是计算图是动态生成的，即边运算边生成计算图，是一个不断完成的过程，可能每运行一行代码都会拓展计算图。动态图便于调试、灵活，静态图速度要高效一些，但是不能改变数据流向。


tensorflow：计算图是静态的计算图，其也是因为张量在预先定义的图中流动而得名tensor flow

Pytorch的计算图就是动态的，几乎每进行一次运算都会拓展原先的计算图。最后生成完成，进行反向传播，当反向传播完成，计算图默认会被清除，即进行前向传播时记录的计算过程会被释放掉。所以，默认情况下，进行一次前向传播后最多只能用生成的计算图进行一次反向传播。

```





**Pytorch张量**

![img](https://i-blog.csdnimg.cn/blog_migrate/f26af2353efe379270c0450f97b55147.png)

```

pytorch张量：Pytorch0.4.0及其之后的版本中，Tensor已经包含Variable的内容，所以它理应包含Variable的所有属性。Pytorch张量一共有如下图所示的8个属性

Variable是 torch.autograd中的数据类型，主要用于封装 Tensor，便于进行自动求导。在Pytorch0.4.0及其之后的版本中，Variable被并入了Tensor里。

torch.autograd.Variable包含以下5个属性：

data：用于存放tensor，是数据本体。

grad：data的梯度值。

grad_fn：记录创建张量时所用的方法（函数——乘法：grad_fn = <MulBackward0>,加法：grad_fn = <AddBackward0>）,用于反向传播的梯度计算之用。grad_fn指向创建Tensor的函数，如果某一个Tensor是由用户创建的，则指向：None。

requires_grad：指示是否需要梯度，对于需要求导的tensor，其requires_grad属性必须为True，即：requires = Ture.自己定义的tensor的requires_grad属性默认为False，神经网络层中的权值w的tensor的requires_grad属性默认为True。

is_ leaf : 指示是否是叶子结点（张量），用户创建的结点称为叶子结点（张量），如X 与 W。is_leaf 为False，即：is_leaf = Ture,的时候，则不是叶子节点,，is_leaf为True的时候是叶子节点。叶子节点在经过反向传播之后梯度值能够得以保留，非叶子节点的梯度值则为：None。

dtype：张量的数据类型，分成浮点，整型和布尔三大类，共9种数据类型，如下表所示（引自pytorch官网），其中32位浮点和64位整型最为常用。

shape : 张量的形状。如（32,3,448,448）就是一个32*3*448*448的4维张量，相当于32张448*448的RGB图像，堆积方式可以通过上述张量的示意图推出。

device : 张量所在设备，GPU 或CPU，张量在GPU上才能用其加速。

https://developer.aliyun.com/article/1644573

with torch.no_grad()：参数更新过程关闭梯度回传
```

